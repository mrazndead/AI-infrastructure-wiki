import { Video } from '../types';

export const videoContent: Video[] = [
  {
    id: 'vid-02',
    youtubeId: 'zduSFxRajkE',
    title: 'Intro to Large Language Models',
    channel: 'Andrej Karpathy',
    views: '4.2M views',
    duration: '1:00:15',
    description: 'The definitive technical introduction to LLMs. Covers pre-training, fine-tuning, and the infrastructure required to build them.',
    tags: ['Education', 'LLMs', 'Fundamentals']
  },
  {
    id: 'vid-04',
    youtubeId: 'Nb2tebYAaOA',
    title: 'Jim Keller: RISC-V, AI Chips, and Moore\'s Law',
    channel: 'Lex Fridman',
    views: '1.8M views',
    duration: '2:55:12',
    description: 'Legendary chip architect Jim Keller (Apple A-series, AMD Zen, Tesla Autopilot) discusses the physics of computation and Tenstorrent.',
    tags: ['Interview', 'Hardware', 'Future']
  },
  {
    id: 'vid-07',
    youtubeId: 'jkrNMKz9pWU',
    title: 'A Hackers\' Guide to Language Models',
    channel: 'Jeremy Howard',
    views: '1.1M views',
    duration: '1:31:25',
    description: 'A practical, code-first guide to how LLMs work, from tokenization to fine-tuning, by the creator of fast.ai.',
    tags: ['Coding', 'Education', 'Practical']
  },
  {
    id: 'vid-08',
    youtubeId: 'bZQun8Y4L2A',
    title: 'State of GPT',
    channel: 'Andrej Karpathy',
    views: '2.5M views',
    duration: '42:30',
    description: 'Karpathy explains the entire pipeline: Pre-training, Supervised Fine-Tuning (SFT), and RLHF, and how they apply to products like ChatGPT.',
    tags: ['Education', 'Training', 'RLHF']
  },
  {
    id: 'vid-10',
    youtubeId: 'eMlx5fFNoYc',
    title: 'But what is a GPT? Visualizing Transformers',
    channel: '3Blue1Brown',
    views: '7.5M views',
    duration: '27:35',
    description: 'The best visual explanation of the Transformer architecture, embeddings, and attention mechanisms available on the internet.',
    tags: ['Math', 'Visuals', 'Theory']
  }
];